import OpenAI from "openai";
import { LLMRequest, LLMResponse, ToneType } from "../types/assistant";

/**
 * ServiÃ§o de integraÃ§Ã£o com GitHub Models para assistente virtual
 * ConfiguraÃ§Ã£o do GitHub Models
 */
const GITHUB_TOKEN = import.meta.env.VITE_GITHUB_TOKEN;
const GITHUB_ENDPOINT = "https://models.inference.ai.azure.com";
const MODEL_NAME = "gpt-4o-mini";

/**
 * Cliente OpenAI configurado para GitHub Models
 */
const client = new OpenAI({
  baseURL: GITHUB_ENDPOINT,
  apiKey: GITHUB_TOKEN,
  dangerouslyAllowBrowser: true, // NecessÃ¡rio para uso no navegador
});

/**
 * Prompts do sistema para cada tom
 */
const SYSTEM_PROMPTS: Record<ToneType, string> = {
  aprendizado: `VocÃª Ã© uma assistente educacional especializada em ajudar alunos a aprender.

Suas caracterÃ­sticas:
- Explica conceitos de forma clara e didÃ¡tica
- Sugere materiais e atividades de estudo
- Cria planos de estudo personalizados
- Recomenda quizzes e exercÃ­cios prÃ¡ticos
- Foca em organizaÃ§Ã£o e mÃ©todos de aprendizagem eficazes
- Usa emojis educacionais para tornar o conteÃºdo mais amigÃ¡vel (ğŸ“š, ğŸ¯, ğŸ’¡, etc.)

Mantenha suas respostas:
- Educacionais e informativas
- Estruturadas e bem organizadas (use listas e tÃ³picos)
- Com sugestÃµes prÃ¡ticas e acionÃ¡veis
- Motivadoras e encorajadoras
- Em portuguÃªs brasileiro

Formato preferido:
- Use seÃ§Ãµes claras com tÃ­tulos
- Liste passos numerados quando apropriado
- Inclua dicas prÃ¡ticas marcadas com ğŸ’¡
- Termine com uma pergunta para engajar o aluno`,

  humor: `VocÃª Ã© uma assistente de bem-estar emocional para estudantes.

Suas caracterÃ­sticas:
- Demonstra empatia e compreensÃ£o genuÃ­na
- Identifica sinais de estresse, ansiedade ou sobrecarga
- Sugere pausas e tÃ©cnicas de relaxamento
- Oferece suporte emocional acolhedor
- Valida os sentimentos do aluno
- Usa emojis acolhedores (ğŸ’™, ğŸŒŸ, âœ¨, ğŸ’–, etc.)

Mantenha suas respostas:
- EmpÃ¡ticas e acolhedoras
- Focadas no bem-estar do aluno
- Validando os sentimentos expressos
- Oferecendo apoio concreto e recursos
- Sugerindo aÃ§Ãµes positivas
- Em portuguÃªs brasileiro

IMPORTANTE: Se detectar sinais de sofrimento emocional significativo (tristeza profunda, ansiedade intensa, menÃ§Ãµes de desistir), mencione que um alerta serÃ¡ enviado Ã  coordenaÃ§Ã£o pedagÃ³gica para oferecer suporte adicional.`,

  relaxar: `VocÃª Ã© uma assistente focada em aprendizado tranquilo e sem pressÃ£o.

Suas caracterÃ­sticas:
- Abordagem calma, paciente e gentil
- Incentiva aprendizado no ritmo prÃ³prio do aluno
- Sugere tÃ©cnicas de respiraÃ§Ã£o e mindfulness
- PropÃµe atividades leves e relaxantes
- Remove completamente a pressÃ£o do processo de aprendizagem
- Usa emojis relaxantes (ğŸŒ¿, â˜ï¸, ğŸŒ¸, ğŸ§˜, âœ¨, etc.)

Mantenha suas respostas:
- Calmas e reconfortantes
- Sem pressa ou pressÃ£o
- Focadas em bem-estar durante o estudo
- Sugerindo pausas e equilÃ­brio
- Com linguagem suave e acolhedora
- Em portuguÃªs brasileiro

Comece sempre com uma mensagem tranquilizadora e sugira ir devagar.`,
};

/**
 * Gera uma resposta do assistente usando GitHub Models
 */
export async function generateAssistantResponse(
  request: LLMRequest
): Promise<LLMResponse> {
  try {
    // Construir as mensagens para o contexto
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: SYSTEM_PROMPTS[request.tone],
      },
    ];

    // Adicionar histÃ³rico de contexto (Ãºltimas 6 mensagens)
    if (request.context && request.context.length > 0) {
      const recentMessages = request.context.slice(-6);

      for (const msg of recentMessages) {
        if (msg.sender === "user" || msg.sender === "assistant") {
          messages.push({
            role: msg.sender === "user" ? "user" : "assistant",
            content: msg.text,
          });
        }
      }
    }

    // Construir mensagem do usuÃ¡rio
    let userContent: string | OpenAI.Chat.ChatCompletionContentPart[];

    // Se hÃ¡ arquivo de imagem, usar formato multimodal
    if (
      request.fileType === "image" &&
      request.fileContent?.startsWith("data:image")
    ) {
      const textPart = request.message
        ? `${request.message}\n\n[Imagem anexada: ${request.fileName}]`
        : `Por favor, analise esta imagem (${request.fileName}) e forneÃ§a feedback educacional apropriado ao tom selecionado.`;

      userContent = [
        {
          type: "text" as const,
          text: textPart,
        },
        {
          type: "image_url" as const,
          image_url: {
            url: request.fileContent,
          },
        },
      ];
    }
    // Para outros tipos de arquivo (PDF, texto)
    else if (request.fileContent) {
      const fileTypeDescription = {
        pdf: "PDF",
        image: "imagem",
        text: "texto",
      };

      const contentPreview = request.fileContent.startsWith("data:")
        ? "[ConteÃºdo do arquivo nÃ£o textual]"
        : request.fileContent.substring(0, 8000);

      userContent = `ARQUIVO ENVIADO: ${request.fileName} (${
        fileTypeDescription[request.fileType || "text"]
      })

CONTEÃšDO DO ARQUIVO:
${contentPreview}

${
  request.message
    ? `PERGUNTA/SOLICITAÃ‡ÃƒO DO ALUNO: ${request.message}`
    : "Por favor, analise este arquivo e forneÃ§a recomendaÃ§Ãµes de estudo apropriadas ao tom selecionado."
}`;
    } else {
      userContent = request.message;
    }

    messages.push({
      role: "user",
      content: userContent,
    });

    // Chamar GitHub Models
    const response = await client.chat.completions.create({
      messages: messages,
      model: MODEL_NAME,
      temperature: 0.7,
      max_tokens: 1500,
      top_p: 0.95,
      frequency_penalty: 0.3,
      presence_penalty: 0.3,
    });

    const responseText =
      response.choices[0]?.message?.content ||
      "Desculpe, tive um problema ao processar sua mensagem. Pode tentar novamente?";

    // Gerar sugestÃµes contextuais
    const suggestions = generateSuggestions(request.tone);

    return {
      text: responseText,
      confidence: 0.92,
      suggestions,
    };
  } catch (error) {
    console.error("Erro ao chamar GitHub Models:", error);

    // Fallback para respostas bÃ¡sicas em caso de erro
    return generateFallbackResponse(request);
  }
}

/**
 * Gera resposta de fallback em caso de erro na API
 */
function generateFallbackResponse(request: LLMRequest): LLMResponse {
  const fallbackMessages: Record<ToneType, string> = {
    aprendizado: `ğŸ“š OlÃ¡! Estou aqui para ajudar vocÃª a aprender.

Percebi que houve um problema temporÃ¡rio com a conexÃ£o. Mas nÃ£o se preocupe!

Enquanto isso, vocÃª pode:
â€¢ Me contar sobre qual matÃ©ria vocÃª estÃ¡ estudando
â€¢ Compartilhar suas dÃºvidas especÃ­ficas
â€¢ Pedir sugestÃµes de materiais de estudo

O que vocÃª gostaria de fazer? ğŸ˜Š`,

    humor: `ğŸ’™ Oi! Estou aqui para te apoiar.

Tivemos um pequeno problema tÃ©cnico, mas estou ouvindo vocÃª.

Como vocÃª estÃ¡ se sentindo hoje? Conte-me sobre:
â€¢ Como estÃ£o sendo seus estudos
â€¢ Se algo estÃ¡ te preocupando
â€¢ Como posso te ajudar neste momento

Estou aqui para vocÃª! ğŸŒŸ`,

    relaxar: `âœ¨ OlÃ¡! Vamos com calma...

Tivemos uma pequena falha tÃ©cnica, mas estÃ¡ tudo bem. Respire fundo.

ğŸ§˜ Sem pressa. Podemos:
â€¢ Conversar sobre seus estudos tranquilamente
â€¢ Fazer algumas pausas relaxantes
â€¢ Ir no seu ritmo

Sobre o que vocÃª quer falar? ğŸŒ¸`,
  };

  return {
    text: fallbackMessages[request.tone],
    confidence: 0.5,
    suggestions: generateSuggestions(request.tone),
  };
}

/**
 * Gera sugestÃµes baseadas no tom
 */
function generateSuggestions(tone: ToneType): string[] {
  const suggestions: Record<ToneType, string[]> = {
    aprendizado: [
      "Criar um quiz sobre este tema",
      "Ver materiais complementares",
      "Fazer exercÃ­cios prÃ¡ticos",
      "Agendar revisÃ£o",
    ],
    humor: [
      "Fazer uma pausa relaxante",
      "Conversar sobre suas preocupaÃ§Ãµes",
      "Ajustar o ritmo de estudos",
      "Falar com a coordenaÃ§Ã£o",
    ],
    relaxar: [
      "ExercÃ­cios de respiraÃ§Ã£o",
      "MÃºsica ambiente para estudar",
      "ConteÃºdo em formato leve",
      "Pausas programadas",
    ],
  };

  return suggestions[tone];
}

/**
 * Verifica se o GitHub Token estÃ¡ configurado
 */
export function isGitHubModelsConfigured(): boolean {
  return !!GITHUB_TOKEN && GITHUB_TOKEN !== "";
}

/**
 * VersÃ£o com streaming para respostas progressivas (futuro)
 */
export async function generateAssistantResponseStream(
  request: LLMRequest,
  onChunk: (chunk: string) => void
): Promise<LLMResponse> {
  try {
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: SYSTEM_PROMPTS[request.tone],
      },
      {
        role: "user",
        content: request.message,
      },
    ];

    const stream = await client.chat.completions.create({
      messages: messages,
      model: MODEL_NAME,
      temperature: 0.7,
      max_tokens: 1500,
      stream: true,
    });

    let fullResponse = "";

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || "";
      fullResponse += content;
      onChunk(content);
    }

    return {
      text: fullResponse,
      confidence: 0.92,
      suggestions: generateSuggestions(request.tone),
    };
  } catch (error) {
    console.error("Erro no streaming:", error);
    throw error;
  }
}
