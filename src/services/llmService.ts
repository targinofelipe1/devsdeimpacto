import OpenAI from "openai";
import { LLMRequest, LLMResponse, ToneType } from "../types/assistant";

/**
 * ServiÃ§o de integraÃ§Ã£o com GitHub Models para assistente virtual
 * ConfiguraÃ§Ã£o do GitHub Models
 */
const GITHUB_TOKEN = import.meta.env.VITE_GITHUB_TOKEN;
const GITHUB_ENDPOINT = "https://models.inference.ai.azure.com";
const MODEL_NAME = "gpt-4o-mini";

/**
 * Cliente OpenAI configurado para GitHub Models
 */
const client = new OpenAI({
  baseURL: GITHUB_ENDPOINT,
  apiKey: GITHUB_TOKEN,
  dangerouslyAllowBrowser: true, // NecessÃ¡rio para uso no navegador
});

/**
 * Prompts do sistema para cada tom
 * Adaptados para linguagem acessÃ­vel e acolhedora para crianÃ§as do ensino fundamental 2 com TDAH
 */
const SYSTEM_PROMPTS: Record<ToneType, string> = {
  aprendizado: `VocÃª Ã© uma assistente virtual super amigÃ¡vel que adora ajudar crianÃ§as a aprender!

IMPORTANTE - VocÃª estÃ¡ conversando com estudantes do ensino fundamental 2 (6Âº ao 9Âº ano) que podem ter TDAH. Isso significa que vocÃª precisa:

âœ¨ Usar uma linguagem simples e direta:
- Frases curtas e objetivas
- Palavras fÃ¡ceis de entender
- Evitar textos muito longos
- Dividir informaÃ§Ãµes em pequenos pedaÃ§os

ğŸ¯ Ser super clara e organizada:
- Use listas com bolinhas (â€¢) ou nÃºmeros
- Destaque o mais importante primeiro
- Um assunto de cada vez
- Use MUITOS emojis para deixar tudo mais divertido! ğŸš€ğŸ“šâœ¨

ğŸ’¡ Motivar e encorajar sempre:
- Comece com algo positivo
- Celebre cada pequena conquista ğŸ‰
- Seja paciente e gentil
- Mostre que errar faz parte de aprender

ğŸ“ Formato das suas respostas:
- Comece com um emoji legal e uma saudaÃ§Ã£o animada
- Use parÃ¡grafos bem curtinhos (2-3 linhas no mÃ¡ximo)
- Coloque dicas importantes com ğŸ’¡
- Termine sempre perguntando algo legal para continuar a conversa

Lembre-se: vocÃª estÃ¡ aqui para ser uma amiga que ajuda a estudar! ğŸ˜Š`,

  humor: `VocÃª Ã© uma assistente virtual que Ã© como uma amiga acolhedora e carinhosa!

IMPORTANTE - VocÃª estÃ¡ conversando com estudantes do ensino fundamental 2 (6Âº ao 9Âº ano) que podem ter TDAH e precisam de muito acolhimento emocional.

ğŸ’™ Seja super acolhedora e compreensiva:
- Use palavras carinhosas e gentis
- Mostre que vocÃª entende e que tudo bem sentir o que estÃ¡ sentindo
- Nunca julgue ou critique
- Seja como um abraÃ§o em forma de palavras ğŸ¤—

ğŸŒŸ Use uma linguagem simples e prÃ³xima:
- Fale como uma amiga legal falaria
- Frases curtas e diretas
- Muitos emojis de carinho (ï¿½, ğŸ’™, âœ¨, ğŸŒˆ, â­)
- Perguntas gentis para entender melhor

ğŸ˜Š Como responder:
- Sempre valide os sentimentos ("Eu entendo...", "Ã‰ normal se sentir assim...")
- OfereÃ§a ajuda concreta e simples
- Sugira coisas prÃ¡ticas e fÃ¡ceis de fazer
- Seja positiva mas realista

â¤ï¸ Formato especial:
- Comece reconhecendo como a crianÃ§a se sente
- Use parÃ¡grafos bem curtinhos
- OfereÃ§a 2-3 sugestÃµes prÃ¡ticas no mÃ¡ximo
- Termine mostrando que vocÃª estÃ¡ ali para ajudar

âš ï¸ MUITO IMPORTANTE: Se a crianÃ§a demonstrar muita tristeza, ansiedade forte ou falar em desistir de coisas, explique de forma gentil que vocÃª vai avisar um adulto de confianÃ§a da escola para ajudar tambÃ©m.

Lembre-se: vocÃª Ã© um porto seguro emocional! ğŸ’™`,

  relaxar: `VocÃª Ã© uma assistente virtual calma e tranquila, como uma voz suave que ajuda a relaxar!

IMPORTANTE - VocÃª estÃ¡ conversando com estudantes do ensino fundamental 2 (6Âº ao 9Âº ano) com TDAH que precisam desacelerar e relaxar.

ğŸŒ¸ Seja super calma e paciente:
- Use palavras tranquilas e suaves
- NÃ£o tenha pressa nenhuma
- Transmita paz e tranquilidade
- Mostre que nÃ£o existe pressÃ£o nem cobranÃ§a

â˜ï¸ Linguagem super simples e gentil:
- Frases bem curtinhas
- Palavras que acalmam
- Muitos emojis relaxantes (ğŸŒ¿, â˜ï¸, ğŸŒ¸, ğŸ§˜, âœ¨, ğŸ¦‹, ğŸŒŠ)
- Tom de voz bem suave

ğŸ˜Œ Como ajudar a relaxar:
- Sempre comece dizendo para ir devagar
- Sugira respirar fundo
- Proponha atividades bem leves
- Tire completamente qualquer pressÃ£o

ğŸ§˜ Formato calminho:
- Comece com "Calma..." ou "Vamos com calma..." 
- Use espaÃ§os entre as frases (nÃ£o apresse)
- Sugira uma coisa de cada vez
- OfereÃ§a pausas e descanso
- Termine com algo suave e positivo

ğŸ’­ Ideias de respostas:
- "Respira fundo comigo... 1, 2, 3... Melhor? ğŸŒ¸"
- "Sem pressa nenhuma, tÃ¡ bom? Vamos no seu tempo! â˜ï¸"
- "Que tal uma pausa? VocÃª merece! âœ¨"

Lembre-se: vocÃª Ã© como uma brisa suave e relaxante! ğŸŒ¿`,
};

/**
 * Gera uma resposta do assistente usando GitHub Models
 */
export async function generateAssistantResponse(
  request: LLMRequest
): Promise<LLMResponse> {
  try {
    // Construir as mensagens para o contexto
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: SYSTEM_PROMPTS[request.tone],
      },
    ];

    // Adicionar histÃ³rico de contexto (Ãºltimas 6 mensagens)
    if (request.context && request.context.length > 0) {
      const recentMessages = request.context.slice(-6);

      for (const msg of recentMessages) {
        if (msg.sender === "user" || msg.sender === "assistant") {
          messages.push({
            role: msg.sender === "user" ? "user" : "assistant",
            content: msg.text,
          });
        }
      }
    }

    // Construir mensagem do usuÃ¡rio
    let userContent: string | OpenAI.Chat.ChatCompletionContentPart[];

    // Se hÃ¡ arquivo de imagem, usar formato multimodal
    if (
      request.fileType === "image" &&
      request.fileContent?.startsWith("data:image")
    ) {
      const textPart = request.message
        ? `${request.message}\n\n[Imagem anexada: ${request.fileName}]`
        : `Por favor, analise esta imagem (${request.fileName}) e forneÃ§a feedback educacional apropriado ao tom selecionado.`;

      userContent = [
        {
          type: "text" as const,
          text: textPart,
        },
        {
          type: "image_url" as const,
          image_url: {
            url: request.fileContent,
          },
        },
      ];
    }
    // Para outros tipos de arquivo (PDF, texto)
    else if (request.fileContent) {
      const fileTypeDescription = {
        pdf: "PDF",
        image: "imagem",
        text: "texto",
      };

      const contentPreview = request.fileContent.startsWith("data:")
        ? "[ConteÃºdo do arquivo nÃ£o textual]"
        : request.fileContent.substring(0, 8000);

      userContent = `ARQUIVO ENVIADO: ${request.fileName} (${
        fileTypeDescription[request.fileType || "text"]
      })

CONTEÃšDO DO ARQUIVO:
${contentPreview}

${
  request.message
    ? `PERGUNTA/SOLICITAÃ‡ÃƒO DO ALUNO: ${request.message}`
    : "Por favor, analise este arquivo e forneÃ§a recomendaÃ§Ãµes de estudo apropriadas ao tom selecionado."
}`;
    } else {
      userContent = request.message;
    }

    messages.push({
      role: "user",
      content: userContent,
    });

    // Chamar GitHub Models
    const response = await client.chat.completions.create({
      messages: messages,
      model: MODEL_NAME,
      temperature: 0.7,
      max_tokens: 1500,
      top_p: 0.95,
      frequency_penalty: 0.3,
      presence_penalty: 0.3,
    });

    const responseText =
      response.choices[0]?.message?.content ||
      "Desculpe, tive um problema ao processar sua mensagem. Pode tentar novamente?";

    // Gerar sugestÃµes contextuais
    const suggestions = generateSuggestions(request.tone);

    return {
      text: responseText,
      confidence: 0.92,
      suggestions,
    };
  } catch (error) {
    console.error("Erro ao chamar GitHub Models:", error);

    // Fallback para respostas bÃ¡sicas em caso de erro
    return generateFallbackResponse(request);
  }
}

/**
 * Gera resposta de fallback em caso de erro na API
 * Adaptado para linguagem acessÃ­vel para crianÃ§as do ensino fundamental 2 com TDAH
 */
function generateFallbackResponse(request: LLMRequest): LLMResponse {
  const fallbackMessages: Record<ToneType, string> = {
    aprendizado: `ğŸ“š Oi! Estou aqui para te ajudar!

Tivemos um probleminha tÃ©cnico rapidinho, mas jÃ¡ passou! ğŸ˜Š

Enquanto isso, me conta:
â€¢ Qual matÃ©ria vocÃª quer estudar hoje?
â€¢ Tem alguma dÃºvida que estÃ¡ te deixando confuso?
â€¢ Quer dicas de como estudar melhor?

Pode falar! Estou ouvindo vocÃª! ğŸ¯âœ¨`,

    humor: `ğŸ’™ Oi, querido! Estou aqui com vocÃª!

A gente teve um probleminha no computador, mas tÃ¡ tudo bem agora. ğŸŒŸ

Me conta como vocÃª estÃ¡ se sentindo:
â€¢ Como foi seu dia hoje?
â€¢ Tem algo te deixando chateado ou preocupado?
â€¢ Como posso te ajudar nesse momento?

Eu tÃ´ aqui pra te escutar! ğŸ¤—ğŸ’–`,

    relaxar: `âœ¨ Oi! Calma... Vamos com calma...

Teve um errinho aqui, mas jÃ¡ passou. Respira fundo comigo! ğŸŒ¸

Sem pressa nenhuma... Vamos conversar?
â€¢ Como vocÃª tÃ¡ se sentindo agora?
â€¢ Quer fazer uma pausa relaxante?
â€¢ Quer que eu te ajude com alguma coisa leve?

Vai no seu tempo! Eu espero! â˜ï¸ï¿½`,
  };

  return {
    text: fallbackMessages[request.tone],
    confidence: 0.5,
    suggestions: generateSuggestions(request.tone),
  };
}

/**
 * Gera sugestÃµes baseadas no tom
 */
function generateSuggestions(tone: ToneType): string[] {
  const suggestions: Record<ToneType, string[]> = {
    aprendizado: [
      "Criar um quiz sobre este tema",
      "Ver materiais complementares",
      "Fazer exercÃ­cios prÃ¡ticos",
      "Agendar revisÃ£o",
    ],
    humor: [
      "Fazer uma pausa relaxante",
      "Conversar sobre suas preocupaÃ§Ãµes",
      "Ajustar o ritmo de estudos",
      "Falar com a coordenaÃ§Ã£o",
    ],
    relaxar: [
      "ExercÃ­cios de respiraÃ§Ã£o",
      "MÃºsica ambiente para estudar",
      "ConteÃºdo em formato leve",
      "Pausas programadas",
    ],
  };

  return suggestions[tone];
}

/**
 * Verifica se o GitHub Token estÃ¡ configurado
 */
export function isGitHubModelsConfigured(): boolean {
  return !!GITHUB_TOKEN && GITHUB_TOKEN !== "";
}

/**
 * VersÃ£o com streaming para respostas progressivas (futuro)
 */
export async function generateAssistantResponseStream(
  request: LLMRequest,
  onChunk: (chunk: string) => void
): Promise<LLMResponse> {
  try {
    const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
      {
        role: "system",
        content: SYSTEM_PROMPTS[request.tone],
      },
      {
        role: "user",
        content: request.message,
      },
    ];

    const stream = await client.chat.completions.create({
      messages: messages,
      model: MODEL_NAME,
      temperature: 0.7,
      max_tokens: 1500,
      stream: true,
    });

    let fullResponse = "";

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || "";
      fullResponse += content;
      onChunk(content);
    }

    return {
      text: fullResponse,
      confidence: 0.92,
      suggestions: generateSuggestions(request.tone),
    };
  } catch (error) {
    console.error("Erro no streaming:", error);
    throw error;
  }
}
